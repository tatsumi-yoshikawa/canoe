<!-- <h1>detail.html</h1>
<h2>Gemini による</h2> -->

<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3D Audio Visualizer & Synth</title>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            background-color: #f0f2f5; /* 明るい背景 */
            font-family: 'Helvetica Neue', Arial, sans-serif;
            user-select: none;
        }

        #canvas-container {
            width: 100vw;
            height: 100vh;
            display: block;
        }

        /* UI Overlay */
        #overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(255, 255, 255, 0.85);
            backdrop-filter: blur(10px);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 10;
            transition: opacity 0.5s ease;
        }

        h1 {
            color: #333;
            font-weight: 200;
            letter-spacing: 2px;
            margin-bottom: 30px;
            text-align: center;
        }

        .btn-group {
            display: flex;
            gap: 20px;
        }

        button {
            padding: 15px 30px;
            font-size: 16px;
            border: none;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            box-shadow: 0 4px 15px rgba(118, 75, 162, 0.4);
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(118, 75, 162, 0.6);
        }

        #status {
            position: absolute;
            bottom: 20px;
            left: 20px;
            color: #888;
            font-size: 12px;
            pointer-events: none;
        }

        #instruction {
            margin-top: 20px;
            font-size: 14px;
            color: #666;
            text-align: center;
            line-height: 1.6;
        }

        .hidden {
            opacity: 0;
            pointer-events: none;
        }
    </style>
</head>
<body>

    <div id="overlay">
        <h1>INTERACTIVE AUDIO FLOW</h1>
        <div class="btn-group">
            <button id="btn-mic">マイク入力で開始</button>
            <button id="btn-sys">PC内部音声で開始</button>
        </div>
        <div id="instruction">
            ※PC内部音声の場合、共有画面で<b>「音声を共有」</b>にチェックを入れてください。<br>
            マウス移動・クリック・キーボード入力で音が鳴ります。
        </div>
    </div>

    <div id="status">Waiting for input...</div>
    <div id="canvas-container"></div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>

    <script>
        /**
         * GLOBAL CONFIGURATION
         */
        const CONFIG = {
            fftSize: 128,          // Audio Analysis resolution (must be power of 2)
            gridRows: 50,          // Depth of history
            gridCols: 32,          // Number of frequency bars
            barWidth: 0.8,
            barSpacing: 1.2,
            rowSpacing: 1.2,
            camSpeed: 0.05
        };

        let audioContext, analyser, dataArray, source;
        let isAudioActive = false;
        
        // Visuals
        let scene, camera, renderer;
        let instancedMesh;
        let dummy = new THREE.Object3D();
        let gridData = []; // 2D array storing amplitude history [row][col]

        // Synth
        let mouseOsc, mouseGain;
        
        /**
         * INITIALIZATION & UI
         */
        const overlay = document.getElementById('overlay');
        const statusDiv = document.getElementById('status');
        
        document.getElementById('btn-mic').addEventListener('click', () => startApp('mic'));
        document.getElementById('btn-sys').addEventListener('click', () => startApp('sys'));

        async function startApp(mode) {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = CONFIG.fftSize;
                const bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);

                let stream;
                if (mode === 'mic') {
                    stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    statusDiv.innerText = "Mode: Microphone Input";
                } else {
                    // System audio capture often requires 'getDisplayMedia' with video
                    // User must check "Share Audio"
                    stream = await navigator.mediaDevices.getDisplayMedia({ video: true, audio: true });
                    statusDiv.innerText = "Mode: System Audio";
                }

                source = audioContext.createMediaStreamSource(stream);
                source.connect(analyser);
                // For system audio, we might need to connect to destination to hear it too, 
                // but usually the browser handles the loopback or mutes it to prevent echo.
                // In this case, we just visualize it. 
                
                if (mode === 'sys') {
                     // Creating a gain node to mute the output to speakers to prevent feedback loop 
                     // if the user selected microphone by mistake, but for system audio we usually want to monitor.
                     // Here we just analyze.
                }

                // Initialize Synth
                initSynth();

                // Initialize Visuals
                initThreeJS();
                
                // Hide UI
                overlay.classList.add('hidden');
                isAudioActive = true;
                
                animate();

            } catch (err) {
                console.error(err);
                alert("エラー: 音声デバイスへのアクセスが拒否されたか、サポートされていません。\n" + err.message);
            }
        }

        /**
         * THREE.JS VISUALIZATION
         */
        function initThreeJS() {
            const container = document.getElementById('canvas-container');
            
            // Scene
            scene = new THREE.Scene();
            scene.background = new THREE.Color(0xf0f2f5);
            // Add Fog for depth effect (fades into background color)
            scene.fog = new THREE.Fog(0xf0f2f5, 10, 70);

            // Camera
            camera = new THREE.PerspectiveCamera(45, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.set(0, 15, 25);
            camera.lookAt(0, 0, -20);

            // Renderer
            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.shadowMap.enabled = true;
            container.appendChild(renderer.domElement);

            // Lighting
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
            scene.add(ambientLight);

            const dirLight = new THREE.DirectionalLight(0xffffff, 0.8);
            dirLight.position.set(10, 20, 10);
            dirLight.castShadow = true;
            scene.add(dirLight);

            // Initialize Grid Data
            // We only use the first 'gridCols' bins from the FFT
            for (let r = 0; r < CONFIG.gridRows; r++) {
                let row = [];
                for (let c = 0; c < CONFIG.gridCols; c++) {
                    row.push(0);
                }
                gridData.push(row);
            }

            // Create Instanced Mesh
            // Total instances = rows * cols
            const geometry = new THREE.BoxGeometry(CONFIG.barWidth, 1, CONFIG.barWidth);
            // Slightly chamfered box looks nicer, but BoxGeometry is lighter.
            
            const material = new THREE.MeshPhysicalMaterial({
                color: 0xffffff,
                metalness: 0.1,
                roughness: 0.2,
                clearcoat: 1.0,
                clearcoatRoughness: 0.1
            });

            const count = CONFIG.gridRows * CONFIG.gridCols;
            instancedMesh = new THREE.InstancedMesh(geometry, material, count);
            instancedMesh.instanceMatrix.setUsage(THREE.DynamicDrawUsage);
            instancedMesh.castShadow = true;
            instancedMesh.receiveShadow = true;
            scene.add(instancedMesh);

            window.addEventListener('resize', onWindowResize, false);
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        }

        /**
         * MAIN ANIMATION LOOP
         */
        function animate() {
            requestAnimationFrame(animate);

            if (!isAudioActive) return;

            // 1. Get Audio Data
            analyser.getByteFrequencyData(dataArray);

            // 2. Update Grid Data (Shift rows back)
            // Remove last row, add new row at start
            gridData.pop();
            
            let newRow = [];
            // Pick frequencies distributed across the range
            // We skip low frequencies (index 0-2) often contain DC offset or rumble
            const step = Math.floor((dataArray.length / 2) / CONFIG.gridCols); 
            
            for (let i = 0; i < CONFIG.gridCols; i++) {
                // Determine value
                const val = dataArray[i * step + 2] || 0;
                newRow.push(val);
            }
            gridData.unshift(newRow);

            // 3. Update Instanced Mesh
            let i = 0;
            const centerOffset = (CONFIG.gridCols * CONFIG.barSpacing) / 2;
            const time = Date.now() * 0.001;

            for (let r = 0; r < CONFIG.gridRows; r++) {
                for (let c = 0; c < CONFIG.gridCols; c++) {
                    const val = gridData[r][c];
                    const normVal = val / 255.0; // 0.0 to 1.0

                    // Position
                    // x centered, z going negative (into screen)
                    const x = (c * CONFIG.barSpacing) - centerOffset;
                    const z = -(r * CONFIG.rowSpacing);
                    
                    // Height based on audio
                    const scaleY = Math.max(0.1, normVal * 8); 
                    const y = scaleY / 2; // Sit on floor

                    dummy.position.set(x, y, z);
                    dummy.scale.set(1, scaleY, 1);
                    dummy.updateMatrix();
                    instancedMesh.setMatrixAt(i, dummy.matrix);

                    // Color - Aesthetic Logic
                    // Hue varies by X (freq) and Time
                    // Lightness varies by Amplitude (Active = Bright)
                    const hue = (c / CONFIG.gridCols) * 0.5 + (time * 0.05); 
                    const saturation = 0.8;
                    const lightness = 0.3 + (normVal * 0.4);
                    
                    const color = new THREE.Color().setHSL(hue % 1, saturation, lightness);
                    instancedMesh.setColorAt(i, color);

                    i++;
                }
            }
            
            instancedMesh.instanceMatrix.needsUpdate = true;
            if (instancedMesh.instanceColor) instancedMesh.instanceColor.needsUpdate = true;

            // 4. Camera & Scene Movement (Dynamic Feel)
            // Gentle sway
            camera.position.x = Math.sin(time * 0.3) * 5;
            camera.lookAt(0, 0, -20);

            // React to Bass (low freq) for global shake/pulse
            const bass = gridData[0][Math.floor(CONFIG.gridCols/2)];
            if(bass > 180) {
                 camera.position.y = 15 + (bass - 180) * 0.01;
            }

            renderer.render(scene, camera);
        }

        /**
         * INTERACTIVE SOUND GENERATION
         */
        function initSynth() {
            // Mouse Theremin
            mouseOsc = audioContext.createOscillator();
            mouseGain = audioContext.createGain();
            
            mouseOsc.type = 'sine';
            mouseOsc.frequency.value = 440;
            mouseOsc.start();
            
            mouseGain.gain.value = 0; // Start silent
            
            // Chain
            mouseOsc.connect(mouseGain);
            mouseGain.connect(audioContext.destination);
            // Also connect synth to analyser so we can see what we play!
            mouseGain.connect(analyser);

            setupInteraction();
        }

        function setupInteraction() {
            // Mouse Move (Theremin)
            document.addEventListener('mousemove', (e) => {
                if (!isAudioActive) return;
                
                const x = e.clientX / window.innerWidth;
                const y = e.clientY / window.innerHeight;

                // Frequency mapped to X (200Hz - 800Hz)
                const freq = 200 + (x * 800);
                mouseOsc.frequency.setTargetAtTime(freq, audioContext.currentTime, 0.1);

                // Volume mapped to Y (Lower screen = louder, somewhat)
                // Actually let's make it volume based on movement speed or just keep it low background
                // Let's toggle volume based on if mouse is moving? 
                // Requirement: "Generate stylish sound". Let's use simple XY pad style.
                // Volume is higher at the bottom.
                const vol = Math.max(0, (1 - y) * 0.1); 
                mouseGain.gain.setTargetAtTime(vol, audioContext.currentTime, 0.1);
            });

            // Click (Bell/Pluck)
            document.addEventListener('mousedown', () => {
                if (!isAudioActive) return;
                playTone('triangle', 600 + Math.random() * 400, 0.5, 0.8);
                createVisualRipple();
            });

            // Typing (Pentatonic Scale)
            const pentatonic = [261.63, 293.66, 329.63, 392.00, 440.00, 523.25, 587.33, 659.25]; // C D E G A C D E
            document.addEventListener('keydown', (e) => {
                if (!isAudioActive) return;
                
                // Map char code to note
                const noteIndex = e.code.length % pentatonic.length;
                // Add octave variation
                const freq = pentatonic[noteIndex] * (Math.random() > 0.8 ? 2 : 1);
                
                playTone('sine', freq, 0.2, 0.3);
            });
        }

        function playTone(type, freq, duration, maxVol) {
            const osc = audioContext.createOscillator();
            const gain = audioContext.createGain();

            osc.type = type;
            osc.frequency.setValueAtTime(freq, audioContext.currentTime);

            gain.gain.setValueAtTime(0, audioContext.currentTime);
            gain.gain.linearRampToValueAtTime(maxVol, audioContext.currentTime + 0.05);
            gain.gain.exponentialRampToValueAtTime(0.001, audioContext.currentTime + duration);

            osc.connect(gain);
            gain.connect(audioContext.destination);
            gain.connect(analyser); // Visualize typing

            osc.start();
            osc.stop(audioContext.currentTime + duration);
        }

        // Simple visual effect for click (change background briefly)
        function createVisualRipple() {
            scene.background.setHex(0xffeef5); // slight pink flash
            setTimeout(() => {
                scene.background.setHex(0xf0f2f5);
            }, 100);
        }

    </script>
</body>
</html>
